{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<strong>IIT ID:</strong> 2019175\n",
    "</p>\n",
    "<p>\n",
    "<strong>Uow ID:</strong> W1790792\n",
    "</p>\n",
    "<p>\n",
    "<strong>Name:</strong> Saadh Jawwadh\n",
    "</p>\n",
    "<p>\n",
    "\n",
    "<!-- Give me a table of contents of h1-->\n",
    "<a href = '#part-a-–-application-area-review'>Part A – Application area review</a>\n",
    "</p>\n",
    "<p>\n",
    "<a href = '#part-b-–-compare-and-evaluate-ai-techniques'>Part B – Compare and evaluate AI techniques</a>\n",
    "</p>\n",
    "\n",
    "\n",
    "<h1 id=\"part-a-–-application-area-review\">Part A – Application area review</h1>\n",
    "\n",
    "\n",
    "<p>\n",
    "Prior to the implementation I did a proper Literature review to acknowledge the state of art techniques for detecting YouTube spam comments and ended up with the following findings.\n",
    "</p>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "   <td><strong>Citation</strong>\n",
    "   </td>\n",
    "   <td><strong>Techniques</strong>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><a href=\"https://doi.org/10.1109/ICMLA.2015.37\">(Alberto, Lochter and Almeida, 2015)</a>\n",
    "   </td>\n",
    "   <td>NB, LR, KNN, RF\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><a href=\"https://doi.org/10.1016/j.procs.2018.05.181\">(Aiyar and Shetty, 2018)</a>\n",
    "   </td>\n",
    "   <td>N-Gram\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><a href=\"https://doi.org/10.1109/ICACCI.2018.8554405\">(Kanodia, Sasheendran and Pathari, 2018)</a>\n",
    "   </td>\n",
    "   <td>Markov’s decision process\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><a href=\"https://www.irjet.net/archives/V7/i4/IRJET-V7I488.pdf\">(Selvaraj, Konatham and Anand, 2020)</a>\n",
    "   </td>\n",
    "   <td>LR\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><a href=\"https://doi.org/10.1109/ACCESS.2021.3121508\">(Oh, 2021)</a>\n",
    "   </td>\n",
    "   <td>Decision Tree, LR, NB, SVM\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><a href=\"https://www.semanticscholar.org/paper/A-COMPARATIVE-STUDY-ON-YOUTUBE-SPAM-COMMENT-USING-Ruth-Khan/644b2af51efb2ddbff0634af974b6c0d5abea30d\">(Ruth, Khan and Reddy, 2022)</a>\n",
    "   </td>\n",
    "   <td>RF, NB, SVM\n",
    "   </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "<p>\n",
    "Even though techniques like Markov’s decision process and N-Gram have been researched and used for this problem <em>(Aiyar and Shetty, 2018; Kanodia, Sasheendran and Pathari, 2018)</em> classification methods like Naive Bayes (NB), Random Forest (RF), Support Vector Machine (SVM), Logistic Regression (LR), K-nearest Neighbors (KNN)  showed a promising results <em>(Alberto, Lochter and Almeida, 2015; Kanodia, Sasheendran and Pathari, 2018; Selvaraj, Konatham and Anand, 2020; Ruth, Khan and Reddy, 2022) </em>which clearly stated that classification is the optimal technique to address this problem.\n",
    "</p>\n",
    "<h1 id=\"part-b-–-compare-and-evaluate-ai-techniques\">Part B – Compare and evaluate AI techniques.</h1>\n",
    "\n",
    "\n",
    "<p>\n",
    "Out of the above state of art techniques I have taken Random Forest (RF), Logistic Regression (LR) and  Support Vector Machine (SVM) to compare and evaluate. \n",
    "</p>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "   <td><strong>Algorithm</strong>\n",
    "   </td>\n",
    "   <td><strong>Strength</strong>\n",
    "   </td>\n",
    "   <td><strong>Weakness</strong>\n",
    "   </td>\n",
    "   <td><strong>Advantage</strong>\n",
    "   </td>\n",
    "   <td><strong>Disadvantage</strong>\n",
    "   </td>\n",
    "   <td><strong>Input</strong>\n",
    "   </td>\n",
    "   <td><strong>Output</strong>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Random Forest (RF)\n",
    "   </td>\n",
    "   <td>Good performance on large datasets, handling missing data and high dimensional spaces, ability to identify important features\n",
    "   </td>\n",
    "   <td>Computationally expensive, not suitable for real-time applications\n",
    "   </td>\n",
    "   <td>Ensemble method that improves overall performance, good for identifying important features\n",
    "   </td>\n",
    "   <td>Computationally expensive, not suitable for real-time applications\n",
    "   </td>\n",
    "   <td>Numerical or categorical features\n",
    "   </td>\n",
    "   <td>Binary class label (spam or not spam)\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Logistic Regression (LR)\n",
    "   </td>\n",
    "   <td>Simple to implement, requires less computational resources, easily interpretable\n",
    "   </td>\n",
    "   <td>Sensitive to outliers, not robust for non-linear problems\n",
    "   </td>\n",
    "   <td>Good for binary classification problems, easy to implement and interpret\n",
    "   </td>\n",
    "   <td>Sensitive to outliers, not robust for non-linear problems\n",
    "   </td>\n",
    "   <td>Numerical or categorical features\n",
    "   </td>\n",
    "   <td>Binary class label (spam or not spam)\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Support Vector Machine (SVM)\n",
    "   </td>\n",
    "   <td>Good for high dimensional spaces and non-linear problems\n",
    "   </td>\n",
    "   <td>Sensitive to choice of kernel, selection of parameters\n",
    "   </td>\n",
    "   <td>Good for classification and regression problems, useful when number of features is greater than number of samples\n",
    "   </td>\n",
    "   <td>Sensitive to choice of kernel, selection of parameters\n",
    "   </td>\n",
    "   <td>Numerical or categorical features\n",
    "   </td>\n",
    "   <td>Class label, boundary that separates the two classes\n",
    "   </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "<p>\n",
    "In the context of YouTube comment spam detection, RFs can be used to classify comments as spam or not spam based on various features such as the text content, user information, and comment history. LR can be used to predict the probability of a comment being spam, based on a set of features, and SVM can be used to build a model that separates spam comments from non-spam comments.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Psy = pd.read_csv('Youtube01-Psy.csv')\n",
    "Katy = pd.read_csv('Youtube02-KatyPerry.csv')\n",
    "Eminem = pd.read_csv('Youtube04-Eminem.csv')\n",
    "Shakira = pd.read_csv('Youtube05-Shakira.csv')\n",
    "LMFAO = pd.read_csv('Youtube03-LMFAO.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all dataframes\n",
    "df = pd.concat([Shakira, Eminem, Katy, Psy, LMFAO])\n",
    "df.drop('DATE', axis=1, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are several ways to check the integrity and cleanliness of your data in Python.\n",
    "\n",
    "+ ***pandas_profiling*** is a library that can generate an HTML report of a DataFrame's content, including information about the data types and missing values. \n",
    "\n",
    "+ ***df.info()*** method in pandas gives a summary of the dataframe, including the number of rows, number of columns, number of non-missing values, and the data types of the columns. \n",
    "\n",
    "+  ***df.describe()*** method in pandas gives the statistical summary of all the columns, including mean, median, standard deviation etc. \n",
    "\n",
    "+  ***df.isnull().sum()*** method in pandas gives the count of missing values in each column.\n",
    "\n",
    "+  ***df.duplicated()*** method in pandas returns a Boolean series indicating whether each element is a duplicate or not.\n",
    "+  ***df.drop_duplicates()*** method in pandas removes duplicate rows from the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop_duplicates()\n",
    "df.duplicated()\n",
    "# df.info()\n",
    "# df.describe()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CLASS'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = df['CLASS']\n",
    "print(classes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_messages = df[\"CONTENT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regular expressions to replace email addresses, URLs, phone numbers, other numbers\n",
    "\n",
    "# Replace email addresses with 'email'\n",
    "processed = text_messages.str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$',\n",
    "                                 'emailaddress')\n",
    "\n",
    "# Replace URLs with 'webaddress'\n",
    "processed = processed.str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$',\n",
    "                                  'webaddress')\n",
    "\n",
    "# Replace money symbols with 'moneysymb' (£ can by typed with ALT key + 156)\n",
    "processed = processed.str.replace(r'£|\\$', 'moneysymb')\n",
    "    \n",
    "# Replace 10 digit phone numbers (formats include paranthesis, spaces, no spaces, dashes) with 'phonenumber'\n",
    "processed = processed.str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$',\n",
    "                                  'phonenumbr')\n",
    "    \n",
    "# Replace numbers with 'numbr'\n",
    "processed = processed.str.replace(r'\\d+(\\.\\d+)?', 'numbr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_messages[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = processed.str.replace(r'[^\\w\\d\\s]', ' ')\n",
    "\n",
    "# Replace whitespace between terms with a single space\n",
    "processed = processed.str.replace(r'\\s+', ' ')\n",
    "\n",
    "# Remove leading and trailing whitespace\n",
    "processed = processed.str.replace(r'^\\s+|\\s+?$', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change words to lower case - Hello, HELLO, hello are all the same word\n",
    "processed = processed.str.lower()\n",
    "print(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# remove stop words from text messages\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "processed = processed.apply(lambda x: ' '.join(\n",
    "    term for term in x.split() if term not in stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove word stems using a Porter stemmer\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "processed = processed.apply(lambda x: ' '.join(\n",
    "    ps.stem(term) for term in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# create bag-of-words\n",
    "all_words = []\n",
    "\n",
    "for message in processed:\n",
    "    words = word_tokenize(message)\n",
    "    for w in words:\n",
    "        all_words.append(w)\n",
    "        \n",
    "all_words = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the total number of words and the 15 most common words\n",
    "print('Number of words: {}'.format(len(all_words)))\n",
    "print('Most common words: {}'.format(all_words.most_common(15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the 1500 most common words as features\n",
    "word_features = list(all_words.keys())[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The find_features function will determine which of the 1500 word features are contained in the review\n",
    "def find_features(message):\n",
    "    words = word_tokenize(message)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Lets see an example!\n",
    "features = find_features(str(processed[0]))\n",
    "for key, value in features.items():\n",
    "    if value == True:\n",
    "        print (key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets do it for all the messages\n",
    "messages = list(zip(processed, classes))\n",
    "\n",
    "# define a seed for reproducibility\n",
    "seed = 1\n",
    "np.random.seed = seed\n",
    "np.random.shuffle(messages)\n",
    "\n",
    "# call find_features function for each SMS message\n",
    "featuresets = [(find_features(text), label) for (text, label) in messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can split the featuresets into training and testing datasets using sklearn\n",
    "from sklearn import model_selection\n",
    "\n",
    "# split the data into training and testing datasets\n",
    "training, testing = model_selection.train_test_split(featuresets, test_size = 0.25, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training))\n",
    "print(len(testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use sklearn algorithms in NLTK\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SklearnClassifier(SVC(kernel = 'linear'))\n",
    "\n",
    "# train the model on the training data\n",
    "model.train(training)\n",
    "\n",
    "# and test on the testing dataset!\n",
    "accuracy = nltk.classify.accuracy(model, testing)*100\n",
    "print(\"SVC Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# Define modemetricso train  \n",
    "names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n",
    "         \"Naive Bayes\", \"SVM Linear\",  \"MLPClassifier\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(max_iter = 100),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel = 'linear'),\n",
    "    MLPClassifier ()\n",
    "]\n",
    "\n",
    "models = zip(names, classifiers)\n",
    "names1 = []\n",
    "results = []\n",
    "\n",
    "for name, model in models:\n",
    "    nltk_model = SklearnClassifier(model)\n",
    "    nltk_model.train(training)\n",
    "    accuracy = nltk.classify.accuracy(nltk_model, testing)*100\n",
    "    print(\"{} Accuracy: {}\".format(name, accuracy))\n",
    "    names1.append(name)\n",
    "    results.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_features, labels = zip(*testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = nltk_model.classify_many(txt_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a confusion matrix and a classification report\n",
    "print(classification_report(labels, prediction))\n",
    "\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(labels, prediction),\n",
    "    index = [['actual', 'actual'], ['ham', 'spam']],\n",
    "    columns = [['predicted', 'predicted'], ['ham', 'spam']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f4f22379b8d6ca0aaa5e64d2ebfc1657349199a8adce4ccf73ce577bcea6099a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
