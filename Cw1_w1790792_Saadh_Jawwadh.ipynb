{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<strong>IIT ID:</strong> 2019175\n",
    "</p>\n",
    "<p>\n",
    "<strong>Uow ID:</strong> W1790792\n",
    "</p>\n",
    "<p>\n",
    "<strong>Name:</strong> Saadh Jawwadh\n",
    "</p>\n",
    "\n",
    "<h1 id=\"part-a-‚Äì-application-area-review\">Part A ‚Äì Application area review</h1>\n",
    "<p>\n",
    "Prior to the implementation I did a proper Literature review to acknowledge the\n",
    "state of art techniques for detecting YouTube spam comments and ended up with\n",
    "the following findings.\n",
    "</p>\n",
    "<table>\n",
    "  <tr>\n",
    "   <td><strong>Citation</strong>\n",
    "   </td>\n",
    "   <td><strong>Techniques</strong>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><a href=\"https://doi.org/10.1109/ICMLA.2015.37\">(Alberto, Lochter and\n",
    "Almeida, 2015)</a>\n",
    "   </td>\n",
    "   <td>NB, LR, KNN, RF\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><a href=\"https://doi.org/10.1016/j.procs.2018.05.181\">(Aiyar and Shetty,\n",
    "2018)</a>\n",
    "   </td>\n",
    "   <td>N-Gram\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><a href=\"https://doi.org/10.1109/ICACCI.2018.8554405\">(Kanodia,\n",
    "Sasheendran and Pathari, 2018)</a>\n",
    "   </td>\n",
    "   <td>Markov‚Äôs decision process\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><a\n",
    "href=\"https://www.irjet.net/archives/V7/i4/IRJET-V7I488.pdf\">(Selvaraj, Konatham\n",
    "and Anand, 2020)</a>\n",
    "   </td>\n",
    "   <td>LR\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><a href=\"https://doi.org/10.1109/ACCESS.2021.3121508\">(Oh, 2021)</a>\n",
    "   </td>\n",
    "   <td>Decision Tree, LR, NB, SVM\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><a\n",
    "href=\"https://www.semanticscholar.org/paper/A-COMPARATIVE-STUDY-ON-YOUTUBE-SPAM-COMMENT-USING-Ruth-Khan/644b2af51efb2ddbff0634af974b6c0d5abea30d\">(Ruth,\n",
    "Khan and Reddy, 2022)</a>\n",
    "   </td>\n",
    "   <td>RF, NB, SVM\n",
    "   </td>\n",
    "  </tr>\n",
    "</table>\n",
    "<p>\n",
    "Even though techniques like Markov‚Äôs decision process and N-Gram have been\n",
    "researched and used for this problem <em>(Aiyar and Shetty, 2018; Kanodia,\n",
    "Sasheendran and Pathari, 2018)</em> classification methods like Naive Bayes\n",
    "(NB), Random Forest (RF), Support Vector Machine (SVM), Logistic Regression\n",
    "(LR), K-nearest Neighbors (KNN)  showed a promising results <em>(Alberto,\n",
    "Lochter and Almeida, 2015; Kanodia, Sasheendran and Pathari, 2018; Selvaraj,\n",
    "Konatham and Anand, 2020; Ruth, Khan and Reddy, 2022) </em>which clearly stated\n",
    "that classification is the optimal technique to address this problem.\n",
    "</p>\n",
    "<h1 id=\"part-b-‚Äì-compare-and-evaluate-ai-techniques\">Part B ‚Äì Compare and\n",
    "evaluate AI techniques.</h1>\n",
    "<p>\n",
    "Out of the above state of art techniques I have taken Random Forest (RF),\n",
    "Logistic Regression (LR) and  Support Vector Machine (SVM) to compare and\n",
    "evaluate.\n",
    "</p>\n",
    "<table>\n",
    "  <tr>\n",
    "   <td><strong>Algorithm</strong>\n",
    "   </td>\n",
    "   <td><strong>Strength</strong>\n",
    "   </td>\n",
    "   <td><strong>Weakness</strong>\n",
    "   </td>\n",
    "   <td><strong>Advantage</strong>\n",
    "   </td>\n",
    "   <td><strong>Disadvantage</strong>\n",
    "   </td>\n",
    "   <td><strong>Input</strong>\n",
    "   </td>\n",
    "   <td><strong>Output</strong>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Random Forest (RF)\n",
    "   </td>\n",
    "   <td>Good performance on large datasets, handling missing data and high\n",
    "dimensional spaces, ability to identify important features\n",
    "   </td>\n",
    "   <td>Computationally expensive, not suitable for real-time applications\n",
    "   </td>\n",
    "   <td>Ensemble method that improves overall performance, good for identifying\n",
    "important features\n",
    "   </td>\n",
    "   <td>Computationally expensive, not suitable for real-time applications\n",
    "   </td>\n",
    "   <td>Numerical or categorical features\n",
    "   </td>\n",
    "   <td>Binary class label (spam or not spam)\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Logistic Regression (LR)\n",
    "   </td>\n",
    "   <td>Simple to implement, requires less computational resources, easily\n",
    "interpretable\n",
    "   </td>\n",
    "   <td>Sensitive to outliers, not robust for non-linear problems\n",
    "   </td>\n",
    "   <td>Good for binary classification problems, easy to implement and interpret\n",
    "   </td>\n",
    "   <td>Sensitive to outliers, not robust for non-linear problems\n",
    "   </td>\n",
    "   <td>Numerical or categorical features\n",
    "   </td>\n",
    "   <td>Binary class label (spam or not spam)\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Support Vector Machine (SVM)\n",
    "   </td>\n",
    "   <td>Good for high dimensional spaces and non-linear problems\n",
    "   </td>\n",
    "   <td>Sensitive to choice of kernel, selection of parameters\n",
    "   </td>\n",
    "   <td>Good for classification and regression problems, useful when number of\n",
    "features is greater than number of samples\n",
    "   </td>\n",
    "   <td>Sensitive to choice of kernel, selection of parameters\n",
    "   </td>\n",
    "   <td>Numerical or categorical features\n",
    "   </td>\n",
    "   <td>Class label, boundary that separates the two classes\n",
    "   </td>\n",
    "  </tr>\n",
    "</table>\n",
    "<p>\n",
    "In the context of YouTube comment spam detection, RFs can be used to classify\n",
    "comments as spam or not spam based on various features such as the text content,\n",
    "user information, and comment history. LR can be used to predict the probability\n",
    "of a comment being spam, based on a set of features, and SVM can be used to\n",
    "build a model that separates spam comments from non-spam comments.\n",
    "</p>\n",
    "<h1 id=\"part-c-‚Äì-implementation\">Part C ‚Äì Implementation.</h1>\n",
    "<h1>\n",
    "<img src=\"AAI.png\" width=\"60%\" alt=\"alt_text\" title=\"image_tooltip\">\n",
    "</h1>\n",
    "<h2 id=\"datasets\">Datasets</h2>\n",
    "<p>\n",
    "This is a public set of comments collected for spam research. It has five\n",
    "datasets composed of 1,956 real messages extracted from five videos that were\n",
    "among the 10 most viewed on the collection period.\n",
    "</p>\n",
    "<table>\n",
    "  <tr>\n",
    "   <td><strong>Dataset </strong>\n",
    "   </td>\n",
    "   <td><strong>YouTube ID</strong>\n",
    "   </td>\n",
    "   <td><strong>Spam </strong>\n",
    "   </td>\n",
    "   <td><strong>Ham </strong>\n",
    "   </td>\n",
    "   <td><strong>Total</strong>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Psy\n",
    "   </td>\n",
    "   <td>9bZkp7q19f0\n",
    "   </td>\n",
    "   <td>175\n",
    "   </td>\n",
    "   <td>175\n",
    "   </td>\n",
    "   <td>350\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>KatyPerry\n",
    "   </td>\n",
    "   <td>9bZkp7q19f0\n",
    "   </td>\n",
    "   <td>175\n",
    "   </td>\n",
    "   <td>175\n",
    "   </td>\n",
    "   <td>350\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>LMFAO\n",
    "   </td>\n",
    "   <td>KQ6zr6kCPj8\n",
    "   </td>\n",
    "   <td>236\n",
    "   </td>\n",
    "   <td>202\n",
    "   </td>\n",
    "   <td>438\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Eminem\n",
    "   </td>\n",
    "   <td>uelHwf8o7_U\n",
    "   </td>\n",
    "   <td>245\n",
    "   </td>\n",
    "   <td>203\n",
    "   </td>\n",
    "   <td>448\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Shakira\n",
    "   </td>\n",
    "   <td>pRpeEdMmmQ0\n",
    "   </td>\n",
    "   <td>174\n",
    "   </td>\n",
    "   <td>196\n",
    "   </td>\n",
    "   <td>370\n",
    "   </td>\n",
    "  </tr>\n",
    "</table>\n",
    "<p>\n",
    "As you can see the data set is balanced in most cases yet you can find a clear\n",
    "bar plot which shows the combined datasets balancing below.\n",
    "</p>\n",
    "<p>\n",
    "Source: <a\n",
    "href=\"https://archive.ics.uci.edu/ml/datasets/YouTube+Spam+Collection#\">UCI\n",
    "Machine Learning Repository: YouTube Spam Collection Data Set</a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "\n",
    "import numpy as np  # Numpy is a library for scientific computing with Python\n",
    "import pandas as pd # Pandas is a library for data manipulation and analysis\n",
    "import matplotlib.pyplot as plt # Matplotlib is a plotting library\n",
    "import nltk # Natural Language Toolkit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data\n",
    "Psy = pd.read_csv('Youtube01-Psy.csv')\n",
    "Katy = pd.read_csv('Youtube02-KatyPerry.csv')\n",
    "Eminem = pd.read_csv('Youtube04-Eminem.csv')\n",
    "Shakira = pd.read_csv('Youtube05-Shakira.csv')\n",
    "LMFAO = pd.read_csv('Youtube03-LMFAO.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1956, 4)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate all data into one DataFrame\n",
    "df = pd.concat([Shakira, Eminem, Katy, Psy, LMFAO])\n",
    "df.drop('DATE', axis=1, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>z13lgffb5w3ddx1ul22qy1wxspy5cpkz504</td>\n",
       "      <td>dharma pal</td>\n",
       "      <td>Nice songÔªø</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>z123dbgb0mqjfxbtz22ucjc5jvzcv3ykj</td>\n",
       "      <td>Tiza Arellano</td>\n",
       "      <td>I love song Ôªø</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>z12quxxp2vutflkxv04cihggzt2azl34pms0k</td>\n",
       "      <td>Pr√¨√±√ße≈õ≈õ √Çli≈õ ≈Å√∏v√™ D√∏m√≠√±√∏ M√¢ƒëi≈õ‚Ñ¢ Ôªø</td>\n",
       "      <td>I love song Ôªø</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z12icv3ysqvlwth2c23eddlykyqut5z1h</td>\n",
       "      <td>Eric Gonzalez</td>\n",
       "      <td>860,000,000 lets make it first female to reach...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z133stly3kete3tly22petvwdpmghrlli</td>\n",
       "      <td>Analena L√≥pez</td>\n",
       "      <td>shakira is best for worldcupÔªø</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>z13lvr4iupatjlrem231yvpxolzvspwdl</td>\n",
       "      <td>Salty Croc</td>\n",
       "      <td>Like this comment for no reasonÔªø</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>z12lxhrqdkyusbkji04cihtrvn3jvxnqszg0k</td>\n",
       "      <td>Bob Orton</td>\n",
       "      <td>love this songÔªø</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>z12xhdjrsxm3v550w22oynsjrnmvjhkvj</td>\n",
       "      <td>LuckyMusiqLive</td>\n",
       "      <td>this song is awesome. these guys are the best....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>z13msngo3qvwx1ym223pehqgouexzdmnm</td>\n",
       "      <td>xXxPWND 420xXx</td>\n",
       "      <td>HOW MANY THUMBS UP FOR LOUIS SAVING THE DAY!?!?Ôªø</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>z120hptrylzqzdsoj04cepaonmuyyr1afj0</td>\n",
       "      <td>Matheus Macedo</td>\n",
       "      <td>NICE :3Ôªø</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1953 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                COMMENT_ID  \\\n",
       "0      z13lgffb5w3ddx1ul22qy1wxspy5cpkz504   \n",
       "1        z123dbgb0mqjfxbtz22ucjc5jvzcv3ykj   \n",
       "2    z12quxxp2vutflkxv04cihggzt2azl34pms0k   \n",
       "3        z12icv3ysqvlwth2c23eddlykyqut5z1h   \n",
       "4        z133stly3kete3tly22petvwdpmghrlli   \n",
       "..                                     ...   \n",
       "433      z13lvr4iupatjlrem231yvpxolzvspwdl   \n",
       "434  z12lxhrqdkyusbkji04cihtrvn3jvxnqszg0k   \n",
       "435      z12xhdjrsxm3v550w22oynsjrnmvjhkvj   \n",
       "436      z13msngo3qvwx1ym223pehqgouexzdmnm   \n",
       "437    z120hptrylzqzdsoj04cepaonmuyyr1afj0   \n",
       "\n",
       "                                 AUTHOR  \\\n",
       "0                            dharma pal   \n",
       "1                         Tiza Arellano   \n",
       "2    Pr√¨√±√ße≈õ≈õ √Çli≈õ ≈Å√∏v√™ D√∏m√≠√±√∏ M√¢ƒëi≈õ‚Ñ¢ Ôªø   \n",
       "3                         Eric Gonzalez   \n",
       "4                         Analena L√≥pez   \n",
       "..                                  ...   \n",
       "433                          Salty Croc   \n",
       "434                           Bob Orton   \n",
       "435                      LuckyMusiqLive   \n",
       "436                      xXxPWND 420xXx   \n",
       "437                      Matheus Macedo   \n",
       "\n",
       "                                               CONTENT  CLASS  \n",
       "0                                           Nice songÔªø      0  \n",
       "1                                        I love song Ôªø      0  \n",
       "2                                        I love song Ôªø      0  \n",
       "3    860,000,000 lets make it first female to reach...      0  \n",
       "4                        shakira is best for worldcupÔªø      0  \n",
       "..                                                 ...    ...  \n",
       "433                   Like this comment for no reasonÔªø      1  \n",
       "434                                    love this songÔªø      0  \n",
       "435  this song is awesome. these guys are the best....      1  \n",
       "436   HOW MANY THUMBS UP FOR LOUIS SAVING THE DAY!?!?Ôªø      1  \n",
       "437                                           NICE :3Ôªø      0  \n",
       "\n",
       "[1953 rows x 4 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Duplicates has been checked and removed\n",
    "df.duplicated()\n",
    "df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>z13lgffb5w3ddx1ul22qy1wxspy5cpkz504</td>\n",
       "      <td>dharma pal</td>\n",
       "      <td>Nice songÔªø</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>z123dbgb0mqjfxbtz22ucjc5jvzcv3ykj</td>\n",
       "      <td>Tiza Arellano</td>\n",
       "      <td>I love song Ôªø</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>z12quxxp2vutflkxv04cihggzt2azl34pms0k</td>\n",
       "      <td>Pr√¨√±√ße≈õ≈õ √Çli≈õ ≈Å√∏v√™ D√∏m√≠√±√∏ M√¢ƒëi≈õ‚Ñ¢ Ôªø</td>\n",
       "      <td>I love song Ôªø</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z12icv3ysqvlwth2c23eddlykyqut5z1h</td>\n",
       "      <td>Eric Gonzalez</td>\n",
       "      <td>860,000,000 lets make it first female to reach...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z133stly3kete3tly22petvwdpmghrlli</td>\n",
       "      <td>Analena L√≥pez</td>\n",
       "      <td>shakira is best for worldcupÔªø</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              COMMENT_ID                              AUTHOR  \\\n",
       "0    z13lgffb5w3ddx1ul22qy1wxspy5cpkz504                          dharma pal   \n",
       "1      z123dbgb0mqjfxbtz22ucjc5jvzcv3ykj                       Tiza Arellano   \n",
       "2  z12quxxp2vutflkxv04cihggzt2azl34pms0k  Pr√¨√±√ße≈õ≈õ √Çli≈õ ≈Å√∏v√™ D√∏m√≠√±√∏ M√¢ƒëi≈õ‚Ñ¢ Ôªø   \n",
       "3      z12icv3ysqvlwth2c23eddlykyqut5z1h                       Eric Gonzalez   \n",
       "4      z133stly3kete3tly22petvwdpmghrlli                       Analena L√≥pez   \n",
       "\n",
       "                                             CONTENT  CLASS  \n",
       "0                                         Nice songÔªø      0  \n",
       "1                                      I love song Ôªø      0  \n",
       "2                                      I love song Ôªø      0  \n",
       "3  860,000,000 lets make it first female to reach...      0  \n",
       "4                      shakira is best for worldcupÔªø      0  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the first 5 rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Counted the instances of each class and plotted them\n",
    "# 1 is spam and 0 is ham (not spam)\n",
    "df['CLASS'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1005\n",
      "0     951\n",
      "Name: CLASS, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "classes = df['CLASS']\n",
    "print(classes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_messages = df[\"CONTENT\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"data-preprocessing\">Data Preprocessing</h2>\n",
    "\n",
    "\n",
    "<ol>\n",
    "\n",
    "<li><strong>Regular expressions have been used to replace sensitive information</strong> like email addresses, URLs, phone numbers, and other numbers.\n",
    "\n",
    "<li><strong>Converting words to lowercase</strong> before processing them is used to standardize and reduce dimensionality of data.\n",
    "\n",
    "<li><strong>Removed stop words</strong> such as \"a\", \"an\", \"the\", \"and\", \"but\" etc. Since they do not carry any meaning and by removing them we can reduce the dimensionality.\n",
    "\n",
    "<li><strong>Stemming</strong> has been done using  Porter stemmer which will reduce the unique words to their base form and will help reduce the dimensionality of data\n",
    "\n",
    "<li>A <strong>bag of words has been created</strong> which represents each text document as a fixed-length vector, where each dimension corresponds to a word from a dictionary. The value at each dimension is the frequency count of the word in the document. This will reduce the complexity by ignoring the grammar and so on.\n",
    "\n",
    "<li>Even though there are 3472 words out of those 1500 words have been selected as <strong>features</strong>. By limiting the feature space to the 1500 most common words, the resulting feature vectors will be more compact and computationally efficient to work with and will help to reduce the noise of the model.\n",
    "\n",
    "<li>And at last the <strong>data has been splitted</strong> into training and testing where 75% of the data has been used for training and 25% is to be used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_19980\\351773825.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  processed = text_messages.str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$',\n",
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_19980\\351773825.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  processed = processed.str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$',\n",
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_19980\\351773825.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  processed = processed.str.replace(r'¬£|\\$', 'moneysymb')\n",
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_19980\\351773825.py:15: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  processed = processed.str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$',\n",
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_19980\\351773825.py:19: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  processed = processed.str.replace(r'\\d+(\\.\\d+)?', 'numbr')\n"
     ]
    }
   ],
   "source": [
    "# use regular expressions to replace email addresses, URLs, phone numbers, other numbers\n",
    "\n",
    "# Replace email addresses with 'email'\n",
    "processed = text_messages.str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$',\n",
    "                                 'emailaddress')\n",
    "\n",
    "# Replace URLs with 'webaddress'\n",
    "processed = processed.str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$',\n",
    "                                  'webaddress')\n",
    "\n",
    "# Replace money symbols with 'moneysymb' (¬£ can by typed with ALT key + 156)\n",
    "processed = processed.str.replace(r'¬£|\\$', 'moneysymb')\n",
    "    \n",
    "# Replace 10 digit phone numbers (formats include paranthesis, spaces, no spaces, dashes) with 'phonenumber'\n",
    "processed = processed.str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$',\n",
    "                                  'phonenumbr')\n",
    "    \n",
    "# Replace numbers with 'numbr'\n",
    "processed = processed.str.replace(r'\\d+(\\.\\d+)?', 'numbr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_19980\\4236589359.py:1: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  print(text_messages[:20])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                            Nice songÔªø\n",
      "1                                         I love song Ôªø\n",
      "2                                         I love song Ôªø\n",
      "3     860,000,000 lets make it first female to reach...\n",
      "4                         shakira is best for worldcupÔªø\n",
      "5                     The best world cup song ever!!!!Ôªø\n",
      "6                                               I loveÔªø\n",
      "7     SEE SOME MORE SONG OPEN GOOGLE AND TYPE Shakir...\n",
      "8                                             Awesome Ôªø\n",
      "9                                     I like shakira..Ôªø\n",
      "10    Shakira - Waka Waka <br />LOVE THIS SONG!!!!!!...\n",
      "11                   Why so many disliked??????!!!!!!üòØÔªø\n",
      "12      I don&#39;t think this song will ever get old Ôªø\n",
      "13                                           Love songÔªø\n",
      "14                                           wery goodÔªø\n",
      "15    Every time I hear this song, I think about Ini...\n",
      "16             Whose watching this in 2015. If so hi-5Ôªø\n",
      "17    I love this song so much &lt;3<br />Keep em&#3...\n",
      "18                   i love this song thumsb up to youÔªø\n",
      "19                                       Waka best oneÔªø\n",
      "Name: CONTENT, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(text_messages[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_19980\\2241229190.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  processed = processed.str.replace(r'[^\\w\\d\\s]', ' ')\n",
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_19980\\2241229190.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  processed = processed.str.replace(r'\\s+', ' ')\n",
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_19980\\2241229190.py:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  processed = processed.str.replace(r'^\\s+|\\s+?$', '')\n"
     ]
    }
   ],
   "source": [
    "processed = processed.str.replace(r'[^\\w\\d\\s]', ' ')\n",
    "\n",
    "# Replace whitespace between terms with a single space\n",
    "processed = processed.str.replace(r'\\s+', ' ')\n",
    "\n",
    "# Remove leading and trailing whitespace\n",
    "processed = processed.str.replace(r'^\\s+|\\s+?$', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                              nice song\n",
      "1                                            i love song\n",
      "2                                            i love song\n",
      "3      numbr numbr numbr lets make it first female to...\n",
      "4                           shakira is best for worldcup\n",
      "                             ...                        \n",
      "433                      like this comment for no reason\n",
      "434                                       love this song\n",
      "435    this song is awesome these guys are the best l...\n",
      "436          how many thumbs up for louis saving the day\n",
      "437                                           nice numbr\n",
      "Name: CONTENT, Length: 1956, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# change words to lower case - Hello, HELLO, hello are all the same word\n",
    "processed = processed.str.lower()\n",
    "print(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# remove stop words from text messages\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "processed = processed.apply(lambda x: ' '.join(\n",
    "    term for term in x.split() if term not in stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove word stems using a Porter stemmer\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "processed = processed.apply(lambda x: ' '.join(\n",
    "    ps.stem(term) for term in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# create bag-of-words\n",
    "all_words = []\n",
    "\n",
    "for message in processed:\n",
    "    words = word_tokenize(message)\n",
    "    for w in words:\n",
    "        all_words.append(w)\n",
    "        \n",
    "all_words = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 3472\n",
      "Most common words: [('numbr', 1133), ('check', 581), ('video', 386), ('song', 340), ('com', 284), ('subscrib', 277), ('like', 272), ('youtub', 272), ('br', 258), ('pleas', 249), ('http', 236), ('love', 220), ('channel', 200), ('music', 157), ('make', 139)]\n"
     ]
    }
   ],
   "source": [
    "# print the total number of words and the 15 most common words\n",
    "print('Number of words: {}'.format(len(all_words)))\n",
    "print('Most common words: {}'.format(all_words.most_common(15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the 1500 most common words as features\n",
    "word_features = list(all_words.keys())[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nice\n",
      "song\n",
      "love\n",
      "numbr\n",
      "watch\n",
      "much\n",
      "check\n",
      "youtub\n",
      "com\n",
      "href\n",
      "http\n",
      "www\n",
      "v\n",
      "amp\n",
      "channel\n",
      "girl\n",
      "xxx\n",
      "also\n",
      "free\n",
      "gener\n",
      "tube\n",
      "talk\n",
      "huh\n"
     ]
    }
   ],
   "source": [
    "# The find_features function will determine which of the 1500 word features are contained in the review\n",
    "def find_features(message):\n",
    "    words = word_tokenize(message)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Lets see an example!\n",
    "features = find_features(str(processed[0]))\n",
    "for key, value in features.items():\n",
    "    if value == True:\n",
    "        print (key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets do it for all the messages\n",
    "messages = list(zip(processed, classes))\n",
    "\n",
    "# define a seed for reproducibility\n",
    "seed = 1\n",
    "np.random.seed = seed\n",
    "np.random.shuffle(messages)\n",
    "\n",
    "# call find_features function for each SMS message\n",
    "featuresets = [(find_features(text), label) for (text, label) in messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can split the feature sets into training and testing datasets using sklearn\n",
    "from sklearn import model_selection\n",
    "\n",
    "# split the data into training and testing datasets\n",
    "training, testing = model_selection.train_test_split(featuresets, test_size = 0.25, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1467\n",
      "489\n"
     ]
    }
   ],
   "source": [
    "print(len(training))\n",
    "print(len(testing))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Prototype</h2>\n",
    "\n",
    "\n",
    "<p>\n",
    "Random Forest has been chosen over the other selected models and initially implementation has been done with that.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy: 96.11451942740287\n"
     ]
    }
   ],
   "source": [
    "# Implementing using RandomForestClassifier since it was the selected model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# using RF\n",
    "model = SklearnClassifier(RandomForestClassifier(n_estimators=100))\n",
    "\n",
    "# train the model on the training data\n",
    "model.train(training)\n",
    "# and test on the testing dataset!\n",
    "accuracy = nltk.classify.accuracy(model, testing)*100\n",
    "print(\"RF Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part D ‚Äì Evaluation and Testing.</h2>\n",
    "\n",
    "\n",
    "<p>\n",
    "To evaluate the RF model the result has been compared with SVM and Logistic Regression.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 96.11451942740287\n",
      "Logistic Regression Accuracy: 95.0920245398773\n",
      "SVM Linear Accuracy: 95.0920245398773\n"
     ]
    }
   ],
   "source": [
    "# To evaluate the model, we will use the classification report and confusion matrix from sklearn\n",
    "# Also we are using other models like Logistic Regression and SVM to compare the results.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Define models to train\n",
    "names = [\"Random Forest\", \"Logistic Regression\", \"SVM Linear\"]\n",
    "\n",
    "classifiers = [\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SVC(kernel = 'linear'),\n",
    "]\n",
    "\n",
    "models = zip(names, classifiers)\n",
    "names1 = []\n",
    "results = []\n",
    "\n",
    "for name, model in models:\n",
    "    nltk_model = SklearnClassifier(model)\n",
    "    nltk_model.train(training)\n",
    "    accuracy = nltk.classify.accuracy(nltk_model, testing)*100\n",
    "    print(\"{} Accuracy: {}\".format(name, accuracy))\n",
    "    names1.append(name)\n",
    "    results.append(accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>\n",
    "And as you can see above the RF model is showing a higher accuracy compared to the SVM and LR models.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_features, labels = zip(*testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = nltk_model.classify_many(txt_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95       250\n",
      "           1       0.97      0.92      0.95       239\n",
      "\n",
      "    accuracy                           0.95       489\n",
      "   macro avg       0.95      0.95      0.95       489\n",
      "weighted avg       0.95      0.95      0.95       489\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">actual</th>\n",
       "      <th>ham</th>\n",
       "      <td>244</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>18</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            predicted     \n",
       "                  ham spam\n",
       "actual ham        244    6\n",
       "       spam        18  221"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print a confusion matrix and a classification report to evaluate the model\n",
    "print(classification_report(labels, prediction))\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(labels, prediction),\n",
    "    index = [['actual', 'actual'], ['ham', 'spam']],\n",
    "    columns = [['predicted', 'predicted'], ['ham', 'spam']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "At last the confusion matrix has been used to define the performance of a classification algorithm. Each row of the matrix represents the instances in a predicted class, while each column represents the instances in an actual class (or vice versa). It also reflects a high matrix score <strong>mostly above 90</strong> and we can conclude that the <strong>model is optimal for this problem.</strong>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?Create requirements.txt file automatically\n",
    "# pip freeze > requirements.txt # Reference: https://stackoverflow.com/questions/31684375/automatically-create-requirements-txt-for-python-project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<em>Special thanks to UCI Machine Learning Repository for the dataset and other resources</em>\n",
    "</p>\n",
    "<p>\n",
    "<em> ¬© Saadh Jawwadh</em>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YTAAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "c94b68cb8485b7798f6db9f9ddc32eff40bc7a8bfd27c0544f291c2e260d4b41"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
